{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shahp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 10288: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9ade61be8772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki_00'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-9ade61be8772>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;31m#infile = sys.argv[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m     \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mext_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#we chose wiki00 file under AO folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[0mposting_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_postinglist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mposting_list_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pldict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9ade61be8772>\u001b[0m in \u001b[0;36mext_document\u001b[1;34m(f_name)\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[0mdocs_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;32mwith\u001b[0m  \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mdata\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mdocs_raw\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'<doc id=\"([0-9]*)\".*?>(.*?)</doc>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m|\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mdocs_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ana\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 10288: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"index_creation\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1PNQhfQgyHpyDvDTfOKhByxoed8bEAB2a\n",
    "\"\"\"\n",
    "\n",
    "#pip install hashedindex\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import bs4 as bs\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "import hashedindex\n",
    "import json\n",
    "import sys\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "def ext_title(f_name):\n",
    "  '''\n",
    "  This function extracts the {docid:title}\n",
    "\n",
    "  '''\n",
    "  docs_title = {}\n",
    "  with  open(f_name, \"r\") as fh:\n",
    "    data  = fh.read()\n",
    "    docs_raw  = { doc.group(1).strip():doc.group(2).strip() for doc in re.finditer(r'<doc id=\"([0-9]*)\".*?title=\"(.*?)\">',data,flags = re.M|re.S)}\n",
    "    docs_title = { k: bs.BeautifulSoup(v,features=\"lxml\").get_text() for k,v in docs_raw.items()}\n",
    "    docs_title= dict((k.lower(), v.lower()) for k,v in docs_title.items()) #changing into lower case to avoid repeatetions\n",
    "\n",
    "  return docs_title\n",
    "\n",
    "def ext_document(f_name):\n",
    "  '''\n",
    "  This function extracts the document id and text for each\n",
    "  document in the file for further processing.\n",
    "\n",
    "  '''\n",
    "  docs_final = {}\n",
    "  with  open(f_name, \"r\") as fh:\n",
    "    data  = fh.read()\n",
    "    docs_raw  = { doc.group(1).strip():doc.group(2).strip() for doc in re.finditer(r'<doc id=\"([0-9]*)\".*?>(.*?)</doc>',data,flags = re.M|re.S)}\n",
    "    docs_final = { k: bs.BeautifulSoup(v,features=\"lxml\").get_text() for k,v in docs_raw.items()}\n",
    "    docs_final= dict((k.lower(), v.lower()) for k,v in docs_final.items()) #changing into lower case to avoid repeatetion\n",
    "\n",
    "  return docs_final\n",
    "\n",
    "def make_postinglist(docs):\n",
    "  '''\n",
    "  This function creates a posting list of the form {term:{docid:tf}}\n",
    "  for all the terms in the list of documents\n",
    "\n",
    "  '''\n",
    "  inverted_index  =  hashedindex.HashedIndex()\n",
    "  for (k,v) in docs.items():\n",
    "    for tokens in nltk.word_tokenize(v):\n",
    "      if tokens not in string.punctuation:\n",
    "        inverted_index.add_term_occurrence(tokens,k)\n",
    "  return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "def get_pldict(documents):\n",
    "  '''\n",
    "  This function will give posting list dictionary of the form {docid:{term:tf}\n",
    "  for all documents\n",
    "\n",
    "  '''\n",
    "\n",
    "  pldict = hashedindex.HashedIndex()\n",
    "  for (k,v) in documents.items():\n",
    "    for tokens in nltk.word_tokenize(v):\n",
    "      if tokens not in string.punctuation:\n",
    "        pldict.add_term_occurrence(k,tokens)\n",
    "\n",
    "\n",
    "  return pldict\n",
    "\n",
    "\n",
    "\n",
    "def ext_title_from(docs):\n",
    "  '''\n",
    "  This function creates a posting list of the form :{docid:{term:tf}}\n",
    "  for all the terms in the titles of all documents\n",
    "\n",
    "  '''\n",
    "  index  =  hashedindex.HashedIndex()\n",
    "  for (k,v) in docs.items():\n",
    "    for tokens in nltk.word_tokenize(v):\n",
    "      if tokens not in string.punctuation:\n",
    "        index.add_term_occurrence(k,tokens)\n",
    "        index[k][tokens] = 10*index[k][tokens] #tittleterms have been assigned 10*termfrequency weightage to give them more importance\n",
    "  return index\n",
    "\n",
    "def ext_tittle_index(docs):\n",
    "  '''\n",
    "  this function will return original postinglist of title terms\n",
    "  in the form {docid : {term : tf}} for all docs\n",
    "  '''\n",
    "  index  =  hashedindex.HashedIndex()\n",
    "  for (k,v) in docs.items():\n",
    "    index.add_term_occurrence(k,v)\n",
    "  return index\n",
    "\n",
    "def save_file(filename, index):\n",
    "  with open(filename, \"w+\") as f:\n",
    "    temp  = { str(k):str(json.dumps(index[k])) for k in index.items()}\n",
    "    j  = json.dumps(temp)\n",
    "    f.write(j)\n",
    "    f.close()\n",
    "\n",
    "def openfile(f):\n",
    "  with open(f, \"r+\") as f:\n",
    "    index=json.load(f)\n",
    "    for word,string in index.items():\n",
    "        index[word]=json.loads(index[word])\n",
    "    return index\n",
    "\n",
    "def main(filename):\n",
    "    \n",
    "    print(1)\n",
    "    infile=filename\n",
    "    #infile = sys.argv[1]\n",
    "\n",
    "    documents = ext_document(infile) #we chose wiki00 file under AO folder\n",
    "    posting_list = make_postinglist(documents)\n",
    "    posting_list_dict = get_pldict(documents)\n",
    "\n",
    "    title = ext_title(infile)\n",
    "    title_pl = ext_title_from(title)\n",
    "    title_index = ext_tittle_index(title)\n",
    "\n",
    "\n",
    "    save_file(\"indwot.json\",posting_list_dict)\n",
    "    save_file(\"polwot.json\",posting_list)\n",
    "    save_file(\"title_pl.json\",title_pl)\n",
    "    save_file(\"title.json\",title_index)\n",
    "\n",
    "    title_pl_upt = openfile(\"title_pl.json\")\n",
    "\n",
    "    for (k,v) in title_pl_upt.items():\n",
    "        for (a,b) in v.items():\n",
    "            try:\n",
    "               posting_list[a][k] = posting_list[a][k] +v[a] #we are updating the termfrequency of title terms to give them more weightage\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for (k,v) in title_pl_upt.items():\n",
    "        for (a,b) in v.items():\n",
    "             try:\n",
    "                posting_list_dict[k][a] = posting_list_dict[k][a] + v[a] #we are updating the termfrequency of title terms to give them more weightage\n",
    "\n",
    "             except:\n",
    "                 pass\n",
    "\n",
    "    save_file(\"indt.json\",posting_list_dict)\n",
    "    save_file(\"polt.json\",posting_list)\n",
    "    print(2)\n",
    "\n",
    "main('wiki_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
